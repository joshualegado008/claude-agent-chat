"""
Integration tests for Sub-Phase 1E
Tests complete system end-to-end
"""

import pytest
import asyncio
from pathlib import Path
import shutil
import uuid
import os

# Set up test environment
os.environ.setdefault('OPENAI_API_KEY', 'test-key')
os.environ.setdefault('ANTHROPIC_API_KEY', 'test-key')

from src.agent_coordinator import AgentCoordinator
from src.data_models import AgentRank


@pytest.fixture
def temp_data_dir(tmp_path, monkeypatch):
    """Create temporary data directory for testing"""
    data_dir = tmp_path / "data"
    data_dir.mkdir()
    (data_dir / "agents").mkdir()
    (data_dir / "performance").mkdir()
    (data_dir / "ratings").mkdir()
    (data_dir / "leaderboard").mkdir()
    (data_dir / "conversations").mkdir()

    # Monkeypatch DataStore to use temp directory
    from src.persistence import DataStore
    original_init = DataStore.__init__

    def custom_init(self, data_dir_param: str = 'data'):
        # Always use our temp directory
        original_init(self, str(data_dir))

    monkeypatch.setattr(DataStore, '__init__', custom_init)

    yield data_dir

    # Cleanup after test
    if data_dir.exists():
        shutil.rmtree(data_dir, ignore_errors=True)


# Test Category 1: Simple Topic Flow
@pytest.mark.asyncio
async def test_simple_topic_flow(temp_data_dir):
    """
    Complete flow: topic → agents → conversation → rating
    """
    coordinator = AgentCoordinator(verbose=False)

    # Get agents for simple topic
    agents, metadata = await coordinator.get_or_create_agents(
        "Tell me about photosynthesis"
    )

    # Verify agents created
    assert len(agents) >= 1, "At least one agent should be created"
    assert metadata['agents_created'] >= 1, "Should have created at least one agent"
    assert any(keyword in agents[0].primary_class.lower() for keyword in ['biology', 'botany', 'science', 'plant']), \
        f"Agent class should be biology-related, got: {agents[0].primary_class}"


# Test Category 2: Agent Reuse (Cache Hit)
@pytest.mark.asyncio
async def test_agent_reuse_cache_hit(temp_data_dir):
    """
    Second conversation reuses agent (cache hit)
    """
    coordinator = AgentCoordinator(verbose=False)

    # First conversation
    agents1, metadata1 = await coordinator.get_or_create_agents(
        "What is photosynthesis?"
    )

    initial_agent_count = len(coordinator.active_agents)
    first_agent_id = agents1[0].agent_id

    # Second conversation with similar topic
    agents2, metadata2 = await coordinator.get_or_create_agents(
        "How does photosynthesis work?"
    )

    # Should reuse agent or create similar number
    # (Deduplication may not be perfect, but should show some reuse)
    assert len(coordinator.active_agents) <= initial_agent_count + 2, \
        "Should not create many new agents for similar topic"


# Test Category 3: Multi-Domain Conversation
@pytest.mark.asyncio
async def test_multi_domain_conversation(temp_data_dir):
    """
    Complex topic requiring multiple agents from different domains
    """
    coordinator = AgentCoordinator(verbose=False)

    agents, metadata = await coordinator.get_or_create_agents(
        "Compare quantum computing with classical computing from computer science and physics perspectives"
    )

    # Should create 1-3 agents
    assert len(agents) >= 1, "Should create at least one agent"
    assert len(agents) <= 4, "Should not create too many agents"
    assert metadata['agents_created'] >= 1, "Should have created at least one agent"


# Test Category 4: Promotion Through Multiple Ratings
@pytest.mark.asyncio
async def test_promotion_through_ratings(temp_data_dir):
    """
    Agent gets promoted through multiple good ratings
    """
    coordinator = AgentCoordinator(verbose=False)

    # Create agent
    agents, _ = await coordinator.get_or_create_agents("physics expert needed")
    agent = agents[0]

    profile = coordinator.rating_system.profiles[agent.agent_id]

    # Should start as NOVICE
    assert profile.current_rank == AgentRank.NOVICE, \
        f"Should start as NOVICE, got: {profile.current_rank}"

    # Rate 3 times with 5/5 (15 points total)
    for i in range(3):
        rating, new_rank = coordinator.rating_system.submit_rating(
            agent_id=agent.agent_id,
            agent_name=agent.name,
            conversation_id=str(uuid.uuid4()),
            helpfulness=5,
            accuracy=5,
            relevance=5,
            clarity=5,
            collaboration=5
        )

    # Should promote to COMPETENT (10 points threshold)
    profile = coordinator.rating_system.profiles[agent.agent_id]
    assert profile.current_rank == AgentRank.COMPETENT, \
        f"Should be COMPETENT after 15 points, got: {profile.current_rank} with {profile.promotion_points} points"
    assert profile.promotion_points >= 10, "Should have at least 10 points"


# Test Category 5: Deduplication Prevents Duplicates
@pytest.mark.asyncio
async def test_deduplication_in_action(temp_data_dir):
    """
    Similar topics don't create excessive duplicate agents
    """
    coordinator = AgentCoordinator(verbose=False)

    # Create first agent
    agents1, metadata1 = await coordinator.get_or_create_agents(
        "I need a specialist in ophthalmology"
    )
    first_agent_count = len(coordinator.active_agents)

    # Try to create very similar agent
    agents2, metadata2 = await coordinator.get_or_create_agents(
        "I need an eye doctor expert"
    )

    # Should not double the number of agents
    assert len(coordinator.active_agents) <= first_agent_count + 2, \
        "Should not create many duplicate agents for similar requests"


# Test Category 6: Lifecycle Tier Updates
@pytest.mark.asyncio
async def test_lifecycle_tier_updates(temp_data_dir):
    """
    Agents move through lifecycle tiers correctly
    """
    coordinator = AgentCoordinator(verbose=False)

    # Create agent
    agents, _ = await coordinator.get_or_create_agents("chemistry expert")
    agent_id = agents[0].agent_id

    # Should be HOT (actively used)
    tier = coordinator.lifecycle_manager.get_tier(agent_id, coordinator.rating_system)
    assert tier.name == 'HOT', f"Should be HOT, got: {tier.name}"

    # Mark as inactive
    coordinator.lifecycle_manager.mark_inactive(agent_id)

    # Should be WARM (recently used)
    tier = coordinator.lifecycle_manager.get_tier(agent_id, coordinator.rating_system)
    assert tier.name == 'WARM', f"Should be WARM, got: {tier.name}"


# Test Category 7: Leaderboard Updates After Ratings
@pytest.mark.asyncio
async def test_leaderboard_reflects_ratings(temp_data_dir):
    """
    Leaderboard reflects latest ratings correctly
    """
    coordinator = AgentCoordinator(verbose=False)

    # Create 3 agents
    topics = ["physics expert", "chemistry expert", "biology expert"]
    agents = []

    for topic in topics:
        agent_list, _ = await coordinator.get_or_create_agents(topic)
        agents.append(agent_list[0])

    # Rate with different scores
    scores = [5, 3, 4]  # physics=5, chemistry=3, biology=4

    for agent, score in zip(agents, scores):
        coordinator.rating_system.submit_rating(
            agent_id=agent.agent_id,
            agent_name=agent.name,
            conversation_id=str(uuid.uuid4()),
            helpfulness=score,
            accuracy=score,
            relevance=score,
            clarity=score,
            collaboration=score
        )

    # Get leaderboard
    leaderboard = coordinator.get_leaderboard(top_n=3)

    # Should be ordered by score
    assert len(leaderboard) == 3, "Should have 3 agents in leaderboard"
    assert leaderboard[0].avg_rating > leaderboard[1].avg_rating, \
        "Leaderboard should be sorted by rating"
    assert leaderboard[1].avg_rating > leaderboard[2].avg_rating, \
        "Leaderboard should be sorted by rating"


# Test Category 8: Persistence Across Restarts
@pytest.mark.asyncio
async def test_persistence_across_restarts(temp_data_dir):
    """
    State persists across coordinator restarts
    """
    # First session: Create agents and rate them
    coordinator1 = AgentCoordinator(verbose=False)

    agents, _ = await coordinator1.get_or_create_agents("mathematics expert needed")
    agent_id = agents[0].agent_id
    agent_name = agents[0].name

    # Rate agent
    coordinator1.rating_system.submit_rating(
        agent_id=agent_id,
        agent_name=agent_name,
        conversation_id=str(uuid.uuid4()),
        helpfulness=5,
        accuracy=5,
        relevance=5,
        clarity=5,
        collaboration=5
    )

    points_before = coordinator1.rating_system.profiles[agent_id].promotion_points

    # Save state
    coordinator1.store.save_agent(agents[0])
    coordinator1.store.save_performance_profile(
        coordinator1.rating_system.profiles[agent_id]
    )

    # Destroy coordinator
    del coordinator1

    # Second session: Load from disk
    coordinator2 = AgentCoordinator(verbose=False)

    # Agent should be loaded
    assert agent_id in coordinator2.active_agents, "Agent should be loaded from disk"
    assert agent_id in coordinator2.rating_system.profiles, \
        "Performance profile should be loaded from disk"

    # Points should be preserved
    points_after = coordinator2.rating_system.profiles[agent_id].promotion_points
    assert points_after == points_before, \
        f"Points should be preserved across restarts: {points_before} -> {points_after}"


# Test Category 9: Statistics Tracking
@pytest.mark.asyncio
async def test_statistics_tracking(temp_data_dir):
    """
    System statistics are tracked correctly
    """
    coordinator = AgentCoordinator(verbose=False)

    # Create and rate multiple agents
    for topic in ["physics expert", "chemistry expert", "biology expert"]:
        agents, _ = await coordinator.get_or_create_agents(topic)
        agent = agents[0]

        coordinator.rating_system.submit_rating(
            agent_id=agent.agent_id,
            agent_name=agent.name,
            conversation_id=str(uuid.uuid4()),
            helpfulness=4,
            accuracy=4,
            relevance=4,
            clarity=4,
            collaboration=4
        )

    # Get statistics
    stats = coordinator.get_statistics()

    assert stats['total_agents'] >= 3, "Should have at least 3 agents"
    assert stats['total_conversations'] == 3, "Should have 3 conversations"
    assert stats['avg_rating'] > 3.5, "Average rating should be > 3.5"
    assert 'by_rank' in stats, "Should have rank distribution"
    assert stats['by_rank']['NOVICE'] >= 3, "Should have at least 3 NOVICE agents"


# Test Category 10: Complete User Journey (End-to-End)
@pytest.mark.asyncio
async def test_complete_user_journey(temp_data_dir):
    """
    Complete flow from topic entry to leaderboard
    """
    coordinator = AgentCoordinator(verbose=False)

    # Step 1: User types topic
    topic = "Tell me about quantum mechanics"

    # Step 2: Get agents (refined, analyzed, created/reused)
    agents, metadata = await coordinator.get_or_create_agents(topic)

    assert len(agents) >= 1, "Should have at least one agent"
    assert 'refined_topic' in metadata, "Should have refined topic"
    assert 'expertise_requirements' in metadata, "Should have expertise requirements"

    # Step 3: Simulate conversation
    conv_id = str(uuid.uuid4())

    # Step 4: Rate agents
    for agent in agents:
        rating, new_rank = coordinator.rating_system.submit_rating(
            agent_id=agent.agent_id,
            agent_name=agent.name,
            conversation_id=conv_id,
            helpfulness=5,
            accuracy=4,
            relevance=5,
            clarity=4,
            collaboration=5
        )

        assert rating is not None, "Rating should be created"

        # Step 5: Check promotion
        profile = coordinator.rating_system.profiles[agent.agent_id]
        assert profile.promotion_points > 0, "Should have earned promotion points"

    # Step 6: View leaderboard
    leaderboard = coordinator.get_leaderboard(top_n=10)
    assert len(leaderboard) >= 1, "Leaderboard should have at least 1 agent"
    assert leaderboard[0].agent_id in [a.agent_id for a in agents], \
        "Top agent should be one of our agents"

    # Step 7: Verify persistence
    profile = coordinator.rating_system.profiles[agents[0].agent_id]
    coordinator.store.save_performance_profile(profile)

    # Should be able to load it back
    loaded_profile = coordinator.store.load_performance_profile(agents[0].agent_id)
    assert loaded_profile.agent_id == agents[0].agent_id, \
        "Loaded profile should match original"
    assert loaded_profile.promotion_points == profile.promotion_points, \
        "Loaded points should match original"


# Run all tests
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short", "-k", "not test_complete_user_journey"])
